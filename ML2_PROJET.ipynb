{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!curl -O http://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip fra-eng.zip\n",
        "!head -n 100000 fra.txt | shuf | split -l 5000\n",
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ro5hTsX7E4f",
        "outputId": "21f33ed3-21be-49ed-ffe2-150c4e7ace1c"
      },
      "id": "1Ro5hTsX7E4f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 6562k  100 6562k    0     0  10.7M      0 --:--:-- --:--:-- --:--:-- 10.7M\n",
            "Archive:  fra-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: fra.txt                 \n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "649666c4-9fb8-42d8-9cd6-e771f10068a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "649666c4-9fb8-42d8-9cd6-e771f10068a1",
        "outputId": "5385b930-b824-4228-a02a-6741d5e1ee05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n## Configuration\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"\n",
        "Title: Character-level recurrent sequence-to-sequence model\n",
        "Author: [fchollet](https://twitter.com/fchollet)\n",
        "Date created: 2017/09/29\n",
        "Last modified: 2020/04/26\n",
        "Description: Character-level recurrent sequence-to-sequence model.\n",
        "Accelerator: GPU\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "## Introduction\n",
        "\n",
        "This example demonstrates how to implement a basic character-level\n",
        "recurrent sequence-to-sequence model. We apply it to translating\n",
        "short English sentences into short French sentences,\n",
        "character-by-character. Note that it is fairly unusual to\n",
        "do character-level machine translation, as word-level\n",
        "models are more common in this domain.\n",
        "\n",
        "**Summary of the algorithm**\n",
        "\n",
        "- We start with input sequences from a domain (e.g. English sentences)\n",
        "    and corresponding target sequences from another domain\n",
        "    (e.g. French sentences).\n",
        "- An encoder LSTM turns input sequences to 2 state vectors\n",
        "    (we keep the last LSTM state and discard the outputs).\n",
        "- A decoder LSTM is trained to turn the target sequences into\n",
        "    the same sequence but offset by one timestep in the future,\n",
        "    a training process called \"teacher forcing\" in this context.\n",
        "    It uses as initial state the state vectors from the encoder.\n",
        "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
        "    given `targets[...t]`, conditioned on the input sequence.\n",
        "- In inference mode, when we want to decode unknown input sequences, we:\n",
        "    - Encode the input sequence into state vectors\n",
        "    - Start with a target sequence of size 1\n",
        "        (just the start-of-sequence character)\n",
        "    - Feed the state vectors and 1-char target sequence\n",
        "        to the decoder to produce predictions for the next character\n",
        "    - Sample the next character using these predictions\n",
        "        (we simply use argmax).\n",
        "    - Append the sampled character to the target sequence\n",
        "    - Repeat until we generate the end-of-sequence character or we\n",
        "        hit the character limit.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "## Setup\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "\n",
        "\"\"\"\n",
        "## Download the data\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"shell\n",
        "!curl -O http://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip fra-eng.zip\n",
        "!split -l 10000 fra.txt\n",
        "!zip -r s2s.zip s2s\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "## Configuration\n",
        "\"\"\"\n",
        "\n",
        "############################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "072951c0-f8f3-43cf-845f-6b927bfc4bb5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "072951c0-f8f3-43cf-845f-6b927bfc4bb5",
        "outputId": "ccc7980d-6e33-4874-d93e-716d1c171da2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n## Prepare the data\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "########################################################################################\n",
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 60  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 200000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "\"\"\"\n",
        "## Prepare the data\n",
        "\"\"\"\n",
        "\n",
        "########################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af8b396b-2eb9-431c-ab8f-0841e816046c",
      "metadata": {
        "id": "af8b396b-2eb9-431c-ab8f-0841e816046c"
      },
      "outputs": [],
      "source": [
        "############################################################################################\n",
        "\n",
        "# Get full data dictionary\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_words = set()\n",
        "target_words = set()\n",
        "data_path = \"fra.txt\"\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split(\"\\t\")\n",
        "    input_text =  [w for w in re.split(\"[^a-z0-9áàÀâÂçÇéÉèêÊëîïôÔœùûü']\", input_text.lower()) if w != '']\n",
        "    target_text = ['<BOS>'] + [w for w in re.split(\"[^a-z0-9áàÀâÂçÇéÉèêÊëîïôÔœùûü]\", target_text.lower()) if w != ''] + ['<EOS>']\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for word in input_text:\n",
        "        if word not in input_words:\n",
        "            input_words.add(word)\n",
        "    for word in target_text:\n",
        "        if word not in target_words:\n",
        "            target_words.add(word)\n",
        "\n",
        "input_words = sorted(list(input_words))\n",
        "target_words = sorted(list(target_words))\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_words)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_words)])\n",
        "num_encoder_tokens = len(input_words)\n",
        "num_decoder_tokens = len(target_words)\n",
        "\n",
        "\n",
        "####################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd5509b9-46a7-4384-833e-4ba55a6703ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd5509b9-46a7-4384-833e-4ba55a6703ae",
        "outputId": "6e1550dc-7e8e-4c99-8b17-9a10ca471d52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 5000\n",
            "Number of unique input tokens: 15910\n",
            "Number of unique output tokens: 25904\n",
            "Max sequence length for inputs: 8\n",
            "Max sequence length for outputs: 15\n"
          ]
        }
      ],
      "source": [
        "################################################################################################\n",
        "# Vectorize the data.\n",
        "data_path = \"xab\"\n",
        "num_samples = 5000\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split(\"\\t\")\n",
        "    input_text =  [w for w in re.split(\"[^a-z0-9áàÀâÂçÇéÉèêÊëîïôÔœùûü']\", input_text.lower()) if w != '']\n",
        "    target_text = ['<BOS>'] + [w for w in re.split(\"[^a-z0-9áàÀâÂçÇéÉèêÊëîïôÔœùûü]\", target_text.lower()) if w != ''] + ['<EOS>']\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "### Fill np arrays with the data\n",
        "\"\"\"\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    # encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "\n",
        "####################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f29b90b1-7706-4bff-8058-11bcd29b2e46",
      "metadata": {
        "id": "f29b90b1-7706-4bff-8058-11bcd29b2e46"
      },
      "outputs": [],
      "source": [
        "################################################################################################\n",
        "\"\"\"\n",
        "## Build the model\n",
        "\"\"\"\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "#################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2579a5f4-d0e5-4daa-b931-15e38eb92ef7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2579a5f4-d0e5-4daa-b931-15e38eb92ef7",
        "outputId": "9861ad2c-8636-4e02-86fd-2244fcb54f1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 5000\n",
            "Number of unique input tokens: 15910\n",
            "Number of unique output tokens: 25904\n",
            "Max sequence length for inputs: 8\n",
            "Max sequence length for outputs: 15\n"
          ]
        }
      ],
      "source": [
        "###################################################################################\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_words = set()\n",
        "target_words = set()\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split(\"\\t\")\n",
        "    input_text =  [w for w in re.split(\"[^a-z0-9áàÀâÂçÇéÉèêÊëîïôÔœùûü']\", input_text.lower()) if w != '']\n",
        "    target_text = ['<BOS>'] + [w for w in re.split(\"[^a-z0-9áàÀâÂçÇéÉèêÊëîïôÔœùûü]\", target_text.lower()) if w != ''] + ['<EOS>']\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for word in input_text:\n",
        "        if word not in input_words:\n",
        "            input_words.add(word)\n",
        "    for word in target_text:\n",
        "        if word not in target_words:\n",
        "            target_words.add(word)\n",
        "\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "### Fill np arrays with the data\n",
        "\"\"\"\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    # encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "\n",
        "################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8189e5e-abd8-451c-92e2-5482777e3130",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a8189e5e-abd8-451c-92e2-5482777e3130",
        "outputId": "b87ac8e0-9c5f-4a6f-e797-2636f30222e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n## Train the model\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "####################################################################################\n",
        "\"\"\"\n",
        "## Build the model\n",
        "\"\"\"\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "\"\"\"\n",
        "## Train the model\n",
        "\"\"\"\n",
        "\n",
        "#####################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47e1a0ab-cdbc-4712-bd1a-7a2c1e9bc1d9",
      "metadata": {
        "id": "47e1a0ab-cdbc-4712-bd1a-7a2c1e9bc1d9"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "\n",
        "def compile():\n",
        "    model.compile(\n",
        "        optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "compile()\n",
        "####################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91b497ad-13d6-4260-810e-9cf9771a61d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91b497ad-13d6-4260-810e-9cf9771a61d3",
        "outputId": "a9282bce-dcf5-4316-c133-d8cb36e798f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "63/63 [==============================] - 18s 167ms/step - loss: 2.8569 - accuracy: 0.0812 - val_loss: 2.6140 - val_accuracy: 0.0848\n",
            "Epoch 2/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 2.5084 - accuracy: 0.0858 - val_loss: 2.6197 - val_accuracy: 0.0861\n",
            "Epoch 3/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 2.4612 - accuracy: 0.0864 - val_loss: 2.6317 - val_accuracy: 0.0871\n",
            "Epoch 4/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 2.4340 - accuracy: 0.0882 - val_loss: 2.6284 - val_accuracy: 0.0919\n",
            "Epoch 5/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 2.4149 - accuracy: 0.0909 - val_loss: 2.6142 - val_accuracy: 0.0920\n",
            "Epoch 6/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 2.3970 - accuracy: 0.0935 - val_loss: 2.6070 - val_accuracy: 0.0916\n",
            "Epoch 7/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 2.3795 - accuracy: 0.0955 - val_loss: 2.5987 - val_accuracy: 0.0966\n",
            "Epoch 8/100\n",
            "63/63 [==============================] - 7s 105ms/step - loss: 2.3632 - accuracy: 0.0983 - val_loss: 2.5823 - val_accuracy: 0.0962\n",
            "Epoch 9/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 2.3443 - accuracy: 0.1008 - val_loss: 2.5912 - val_accuracy: 0.0980\n",
            "Epoch 10/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 2.3074 - accuracy: 0.1060 - val_loss: 2.5275 - val_accuracy: 0.1086\n",
            "Epoch 11/100\n",
            "63/63 [==============================] - 7s 105ms/step - loss: 2.2639 - accuracy: 0.1123 - val_loss: 2.4920 - val_accuracy: 0.1129\n",
            "Epoch 12/100\n",
            "63/63 [==============================] - 7s 105ms/step - loss: 2.2261 - accuracy: 0.1169 - val_loss: 2.4871 - val_accuracy: 0.1143\n",
            "Epoch 13/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 2.1899 - accuracy: 0.1225 - val_loss: 2.4623 - val_accuracy: 0.1158\n",
            "Epoch 14/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 2.1580 - accuracy: 0.1282 - val_loss: 2.4312 - val_accuracy: 0.1237\n",
            "Epoch 15/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 2.1265 - accuracy: 0.1324 - val_loss: 2.4167 - val_accuracy: 0.1269\n",
            "Epoch 16/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 2.1007 - accuracy: 0.1352 - val_loss: 2.3929 - val_accuracy: 0.1309\n",
            "Epoch 17/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 2.0762 - accuracy: 0.1392 - val_loss: 2.3981 - val_accuracy: 0.1342\n",
            "Epoch 18/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 2.0542 - accuracy: 0.1410 - val_loss: 2.3885 - val_accuracy: 0.1324\n",
            "Epoch 19/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 2.0321 - accuracy: 0.1434 - val_loss: 2.3650 - val_accuracy: 0.1367\n",
            "Epoch 20/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 2.0121 - accuracy: 0.1461 - val_loss: 2.3633 - val_accuracy: 0.1399\n",
            "Epoch 21/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 1.9913 - accuracy: 0.1491 - val_loss: 2.3595 - val_accuracy: 0.1409\n",
            "Epoch 22/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.9729 - accuracy: 0.1515 - val_loss: 2.3600 - val_accuracy: 0.1394\n",
            "Epoch 23/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 1.9522 - accuracy: 0.1543 - val_loss: 2.3795 - val_accuracy: 0.1366\n",
            "Epoch 24/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.9346 - accuracy: 0.1555 - val_loss: 2.3641 - val_accuracy: 0.1431\n",
            "Epoch 25/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.9146 - accuracy: 0.1580 - val_loss: 2.3595 - val_accuracy: 0.1413\n",
            "Epoch 26/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.8957 - accuracy: 0.1602 - val_loss: 2.3402 - val_accuracy: 0.1471\n",
            "Epoch 27/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.8770 - accuracy: 0.1633 - val_loss: 2.3631 - val_accuracy: 0.1483\n",
            "Epoch 28/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.8586 - accuracy: 0.1647 - val_loss: 2.3594 - val_accuracy: 0.1473\n",
            "Epoch 29/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.8393 - accuracy: 0.1684 - val_loss: 2.3365 - val_accuracy: 0.1516\n",
            "Epoch 30/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.8204 - accuracy: 0.1705 - val_loss: 2.3223 - val_accuracy: 0.1534\n",
            "Epoch 31/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.8045 - accuracy: 0.1726 - val_loss: 2.3436 - val_accuracy: 0.1521\n",
            "Epoch 32/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.7845 - accuracy: 0.1747 - val_loss: 2.3223 - val_accuracy: 0.1538\n",
            "Epoch 33/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 1.7670 - accuracy: 0.1776 - val_loss: 2.3329 - val_accuracy: 0.1531\n",
            "Epoch 34/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.7511 - accuracy: 0.1787 - val_loss: 2.3156 - val_accuracy: 0.1551\n",
            "Epoch 35/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.7355 - accuracy: 0.1807 - val_loss: 2.3134 - val_accuracy: 0.1576\n",
            "Epoch 36/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.7183 - accuracy: 0.1834 - val_loss: 2.3485 - val_accuracy: 0.1550\n",
            "Epoch 37/100\n",
            "63/63 [==============================] - 7s 105ms/step - loss: 1.7026 - accuracy: 0.1852 - val_loss: 2.3154 - val_accuracy: 0.1583\n",
            "Epoch 38/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.6866 - accuracy: 0.1871 - val_loss: 2.3182 - val_accuracy: 0.1574\n",
            "Epoch 39/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.6700 - accuracy: 0.1888 - val_loss: 2.3472 - val_accuracy: 0.1570\n",
            "Epoch 40/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.6551 - accuracy: 0.1913 - val_loss: 2.3023 - val_accuracy: 0.1573\n",
            "Epoch 41/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.6390 - accuracy: 0.1933 - val_loss: 2.3061 - val_accuracy: 0.1599\n",
            "Epoch 42/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.6238 - accuracy: 0.1949 - val_loss: 2.3354 - val_accuracy: 0.1586\n",
            "Epoch 43/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.6089 - accuracy: 0.1967 - val_loss: 2.3195 - val_accuracy: 0.1604\n",
            "Epoch 44/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.5942 - accuracy: 0.1993 - val_loss: 2.3075 - val_accuracy: 0.1595\n",
            "Epoch 45/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.5808 - accuracy: 0.2009 - val_loss: 2.2896 - val_accuracy: 0.1601\n",
            "Epoch 46/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.5645 - accuracy: 0.2036 - val_loss: 2.3011 - val_accuracy: 0.1604\n",
            "Epoch 47/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.5514 - accuracy: 0.2054 - val_loss: 2.3264 - val_accuracy: 0.1594\n",
            "Epoch 48/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.5371 - accuracy: 0.2073 - val_loss: 2.3421 - val_accuracy: 0.1616\n",
            "Epoch 49/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.5226 - accuracy: 0.2096 - val_loss: 2.2990 - val_accuracy: 0.1634\n",
            "Epoch 50/100\n",
            "63/63 [==============================] - 7s 104ms/step - loss: 1.5083 - accuracy: 0.2124 - val_loss: 2.3381 - val_accuracy: 0.1618\n",
            "Epoch 51/100\n",
            "63/63 [==============================] - 7s 109ms/step - loss: 1.4944 - accuracy: 0.2142 - val_loss: 2.3361 - val_accuracy: 0.1630\n",
            "Epoch 52/100\n",
            "63/63 [==============================] - 7s 105ms/step - loss: 1.4809 - accuracy: 0.2170 - val_loss: 2.3169 - val_accuracy: 0.1615\n",
            "Epoch 53/100\n",
            "63/63 [==============================] - 7s 105ms/step - loss: 1.4678 - accuracy: 0.2189 - val_loss: 2.3158 - val_accuracy: 0.1639\n",
            "Epoch 54/100\n",
            "63/63 [==============================] - 7s 105ms/step - loss: 1.4527 - accuracy: 0.2214 - val_loss: 2.3290 - val_accuracy: 0.1633\n",
            "Epoch 55/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.4388 - accuracy: 0.2242 - val_loss: 2.3364 - val_accuracy: 0.1653\n",
            "Epoch 56/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.4256 - accuracy: 0.2260 - val_loss: 2.3155 - val_accuracy: 0.1634\n",
            "Epoch 57/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.4127 - accuracy: 0.2287 - val_loss: 2.3193 - val_accuracy: 0.1664\n",
            "Epoch 58/100\n",
            "63/63 [==============================] - 7s 105ms/step - loss: 1.3990 - accuracy: 0.2304 - val_loss: 2.3566 - val_accuracy: 0.1649\n",
            "Epoch 59/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.3870 - accuracy: 0.2332 - val_loss: 2.3713 - val_accuracy: 0.1658\n",
            "Epoch 60/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.3739 - accuracy: 0.2356 - val_loss: 2.3152 - val_accuracy: 0.1664\n",
            "Epoch 61/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.3603 - accuracy: 0.2374 - val_loss: 2.3766 - val_accuracy: 0.1664\n",
            "Epoch 62/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.3478 - accuracy: 0.2408 - val_loss: 2.3579 - val_accuracy: 0.1659\n",
            "Epoch 63/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.3369 - accuracy: 0.2418 - val_loss: 2.3279 - val_accuracy: 0.1658\n",
            "Epoch 64/100\n",
            "63/63 [==============================] - 7s 105ms/step - loss: 1.3238 - accuracy: 0.2447 - val_loss: 2.3476 - val_accuracy: 0.1669\n",
            "Epoch 65/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.3096 - accuracy: 0.2480 - val_loss: 2.3360 - val_accuracy: 0.1657\n",
            "Epoch 66/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.2981 - accuracy: 0.2493 - val_loss: 2.3732 - val_accuracy: 0.1683\n",
            "Epoch 67/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.2864 - accuracy: 0.2517 - val_loss: 2.3525 - val_accuracy: 0.1669\n",
            "Epoch 68/100\n",
            "63/63 [==============================] - 7s 105ms/step - loss: 1.2739 - accuracy: 0.2542 - val_loss: 2.3669 - val_accuracy: 0.1676\n",
            "Epoch 69/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.2615 - accuracy: 0.2563 - val_loss: 2.3507 - val_accuracy: 0.1673\n",
            "Epoch 70/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.2515 - accuracy: 0.2582 - val_loss: 2.3677 - val_accuracy: 0.1677\n",
            "Epoch 71/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.2384 - accuracy: 0.2610 - val_loss: 2.3888 - val_accuracy: 0.1684\n",
            "Epoch 72/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.2274 - accuracy: 0.2631 - val_loss: 2.3748 - val_accuracy: 0.1697\n",
            "Epoch 73/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.2159 - accuracy: 0.2650 - val_loss: 2.3587 - val_accuracy: 0.1681\n",
            "Epoch 74/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.2062 - accuracy: 0.2673 - val_loss: 2.3855 - val_accuracy: 0.1691\n",
            "Epoch 75/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.1934 - accuracy: 0.2697 - val_loss: 2.3617 - val_accuracy: 0.1665\n",
            "Epoch 76/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.1842 - accuracy: 0.2716 - val_loss: 2.3966 - val_accuracy: 0.1683\n",
            "Epoch 77/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.1736 - accuracy: 0.2731 - val_loss: 2.3679 - val_accuracy: 0.1691\n",
            "Epoch 78/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.1634 - accuracy: 0.2756 - val_loss: 2.3937 - val_accuracy: 0.1674\n",
            "Epoch 79/100\n",
            "63/63 [==============================] - 7s 108ms/step - loss: 1.1519 - accuracy: 0.2781 - val_loss: 2.3970 - val_accuracy: 0.1679\n",
            "Epoch 80/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.1414 - accuracy: 0.2794 - val_loss: 2.3714 - val_accuracy: 0.1694\n",
            "Epoch 81/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.1322 - accuracy: 0.2822 - val_loss: 2.3917 - val_accuracy: 0.1694\n",
            "Epoch 82/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.1215 - accuracy: 0.2828 - val_loss: 2.3750 - val_accuracy: 0.1702\n",
            "Epoch 83/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.1128 - accuracy: 0.2844 - val_loss: 2.4118 - val_accuracy: 0.1702\n",
            "Epoch 84/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.1014 - accuracy: 0.2874 - val_loss: 2.4195 - val_accuracy: 0.1700\n",
            "Epoch 85/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.0924 - accuracy: 0.2891 - val_loss: 2.3896 - val_accuracy: 0.1702\n",
            "Epoch 86/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.0832 - accuracy: 0.2906 - val_loss: 2.3907 - val_accuracy: 0.1692\n",
            "Epoch 87/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.0729 - accuracy: 0.2936 - val_loss: 2.4413 - val_accuracy: 0.1704\n",
            "Epoch 88/100\n",
            "63/63 [==============================] - 7s 105ms/step - loss: 1.0642 - accuracy: 0.2943 - val_loss: 2.3931 - val_accuracy: 0.1703\n",
            "Epoch 89/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.0547 - accuracy: 0.2958 - val_loss: 2.4189 - val_accuracy: 0.1704\n",
            "Epoch 90/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.0465 - accuracy: 0.2975 - val_loss: 2.4117 - val_accuracy: 0.1714\n",
            "Epoch 91/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.0380 - accuracy: 0.2994 - val_loss: 2.3964 - val_accuracy: 0.1707\n",
            "Epoch 92/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 1.0291 - accuracy: 0.3015 - val_loss: 2.4614 - val_accuracy: 0.1712\n",
            "Epoch 93/100\n",
            "63/63 [==============================] - 7s 105ms/step - loss: 1.0203 - accuracy: 0.3038 - val_loss: 2.4300 - val_accuracy: 0.1710\n",
            "Epoch 94/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 1.0114 - accuracy: 0.3047 - val_loss: 2.4097 - val_accuracy: 0.1696\n",
            "Epoch 95/100\n",
            "63/63 [==============================] - 7s 105ms/step - loss: 1.0049 - accuracy: 0.3063 - val_loss: 2.4109 - val_accuracy: 0.1697\n",
            "Epoch 96/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 0.9948 - accuracy: 0.3086 - val_loss: 2.4360 - val_accuracy: 0.1686\n",
            "Epoch 97/100\n",
            "63/63 [==============================] - 7s 104ms/step - loss: 0.9868 - accuracy: 0.3095 - val_loss: 2.4098 - val_accuracy: 0.1696\n",
            "Epoch 98/100\n",
            "63/63 [==============================] - 7s 107ms/step - loss: 0.9789 - accuracy: 0.3109 - val_loss: 2.4708 - val_accuracy: 0.1713\n",
            "Epoch 99/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 0.9716 - accuracy: 0.3127 - val_loss: 2.4335 - val_accuracy: 0.1696\n",
            "Epoch 100/100\n",
            "63/63 [==============================] - 7s 106ms/step - loss: 0.9621 - accuracy: 0.3142 - val_loss: 2.4749 - val_accuracy: 0.1716\n"
          ]
        }
      ],
      "source": [
        "################################################################################\n",
        "\n",
        "def train():\n",
        "    model.fit(\n",
        "        [encoder_input_data, decoder_input_data],\n",
        "        decoder_target_data,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_split=0.2,\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "\n",
        "train()\n",
        "\n",
        "################################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"s2s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYwXvFqfE1ji",
        "outputId": "90986969-fca8-4a81-ea1c-19c0a439d92a"
      },
      "id": "RYwXvFqfE1ji",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r s2s.zip s2s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4xh2jGjE32e",
        "outputId": "dd344672-9a80-4fd4-a1b9-364f936d0872"
      },
      "id": "D4xh2jGjE32e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: s2s/ (stored 0%)\n",
            "  adding: s2s/variables/ (stored 0%)\n",
            "  adding: s2s/variables/variables.index (deflated 62%)\n",
            "  adding: s2s/variables/variables.data-00000-of-00001 (deflated 45%)\n",
            "  adding: s2s/assets/ (stored 0%)\n",
            "  adding: s2s/saved_model.pb (deflated 91%)\n",
            "  adding: s2s/keras_metadata.pb (deflated 90%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip s2s.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQNxGtSeMCOz",
        "outputId": "d8adac63-16ed-4a06-8763-ed3cf6f16162"
      },
      "id": "RQNxGtSeMCOz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  s2s.zip\n",
            "   creating: s2s/\n",
            "   creating: s2s/variables/\n",
            "  inflating: s2s/variables/variables.index  \n",
            "  inflating: s2s/variables/variables.data-00000-of-00001  \n",
            "   creating: s2s/assets/\n",
            "  inflating: s2s/saved_model.pb      \n",
            "  inflating: s2s/keras_metadata.pb   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B6wx2uvfQAHi"
      },
      "id": "B6wx2uvfQAHi"
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"s2s\")"
      ],
      "metadata": {
        "id": "5HGtHndz72JK"
      },
      "id": "5HGtHndz72JK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71af99ee-2018-4f94-9c6a-77ca18a22347",
      "metadata": {
        "id": "71af99ee-2018-4f94-9c6a-77ca18a22347"
      },
      "outputs": [],
      "source": [
        "####################################################################################\n",
        "\n",
        "\"\"\"\n",
        "## Run inference (sampling)\n",
        "\n",
        "1. encode input and retrieve initial decoder state\n",
        "2. run one step of decoder with this initial state\n",
        "and a \"start of sequence\" token as target.\n",
        "Output will be the next target token.\n",
        "3. Repeat with the current target token and current states\n",
        "\"\"\"\n",
        "\n",
        "# Define sampling models\n",
        "# Restore the model and construct the encoder and decoder.\n",
        "model = keras.models.load_model(\"s2s\")\n",
        "\n",
        "encoder_inputs = model.input[0]  # input_1\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model.layers[3]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[4]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "######################################\n",
        "\n",
        "################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31ee744c-d404-4e38-8714-f19e96cd0ed3",
      "metadata": {
        "id": "31ee744c-d404-4e38-8714-f19e96cd0ed3"
      },
      "outputs": [],
      "source": [
        "##############################################\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[\"<BOS>\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = []\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence.append(sampled_char)\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"<EOS>\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence\n",
        "\n",
        "####################\n",
        "\n",
        "####################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[\"<BOS>\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = []\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence.append(sampled_char)\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"<EOS>\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "-cr7DLu9Iko2"
      },
      "id": "-cr7DLu9Iko2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9c35364-793a-45a8-8974-3a6d3b883e0f",
      "metadata": {
        "id": "d9c35364-793a-45a8-8974-3a6d3b883e0f"
      },
      "outputs": [],
      "source": [
        "############################################################################\n",
        "\n",
        "# print(tfa.seq2seq.BeamSearchDecoder(cell=decoder_lstm, beam_width=10, output_layer=decoder_dense))\n",
        "\n",
        "####################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92463ff1-310f-4beb-837c-30cc60de0bb9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92463ff1-310f-4beb-837c-30cc60de0bb9",
        "outputId": "37b34dfe-f322-4888-9a42-12f30f039412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 367ms/step\n",
            "1/1 [==============================] - 0s 325ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "-\n",
            "Input sentence: ['i', 'want', 'to', 'live', 'in', 'the', 'city']\n",
            "Decoded sentence: ['je', 'veux', 'vivre', 'de', '<EOS>']\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "-\n",
            "Input sentence: ['i', 'have', 'a', 'headache']\n",
            "Decoded sentence: ['j', 'ai', 'un', 'de', 'de', '<EOS>']\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "-\n",
            "Input sentence: ['the', 'game', \"isn't\", 'over']\n",
            "Decoded sentence: ['la', 'partie', 'n', 'est', 'pas', 'pas', '<EOS>']\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "-\n",
            "Input sentence: ['he', 'can', 'swim', 'well']\n",
            "Decoded sentence: ['il', 'nage', 'le', '<EOS>']\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "-\n",
            "Input sentence: ['tom', 'can', 'come', 'back', 'tomorrow']\n",
            "Decoded sentence: ['tom', 'peut', 'revenir', '<EOS>']\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "-\n",
            "Input sentence: ['i', 'thought', \"we'd\", 'be', 'safe', 'here']\n",
            "Decoded sentence: ['je', 'pensais', 'que', 'nous', 'serions', 'nous', '<EOS>']\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "-\n",
            "Input sentence: [\"there's\", 'no', 'more', 'salt']\n",
            "Decoded sentence: ['il', 'n', 'y', 'a', 'pas', 'de', 'de', '<EOS>']\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "-\n",
            "Input sentence: [\"i'm\", 'overweight']\n",
            "Decoded sentence: ['je', 'suis', 'en', 'en', '<EOS>']\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "-\n",
            "Input sentence: ['have', 'you', 'decided']\n",
            "Decoded sentence: ['t', 'es', 'tu', 'tu', '<EOS>']\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "-\n",
            "Input sentence: [\"i'm\", 'downstairs']\n",
            "Decoded sentence: ['je', 'suis', 'en', '<EOS>']\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "-\n",
            "Input sentence: ['i', 'bought', 'a', 'book', 'yesterday']\n",
            "Decoded sentence: ['j', 'ai', 'acheté', 'un', 'un', '<EOS>']\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "-\n",
            "Input sentence: ['do', 'you', 'like', 'your', 'classmates']\n",
            "Decoded sentence: ['aimes', 'tu', 'tes', 'tes', '<EOS>']\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "-\n",
            "Input sentence: ['i', 'ordered', 'you', 'a', 'drink']\n",
            "Decoded sentence: ['je', 't', 'ai', 'commandé', 'un', 'de', '<EOS>']\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "-\n",
            "Input sentence: ['tom', 'did', 'do', 'that']\n",
            "Decoded sentence: ['tom', 'est', 'ce', 'que', 'il', 'a', '<EOS>']\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "-\n",
            "Input sentence: ['i', \"didn't\", 'get', 'discouraged']\n",
            "Decoded sentence: ['je', 'ne', 'me', 'suis', 'pas', '<EOS>']\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "-\n",
            "Input sentence: ['i', 'am', 'afraid', 'of', 'dogs']\n",
            "Decoded sentence: ['j', 'ai', 'peur', 'des', '<EOS>']\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "-\n",
            "Input sentence: ['where', 'are', 'we', 'exactly']\n",
            "Decoded sentence: ['où', 'sommes', 'nous', 'nous', '<EOS>']\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "-\n",
            "Input sentence: ['i', \"don't\", 'have', 'a', 'bicycle']\n",
            "Decoded sentence: ['je', 'n', 'ai', 'pas', 'de', 'de', '<EOS>']\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "-\n",
            "Input sentence: ['i', 'made', 'that', 'one']\n",
            "Decoded sentence: ['j', 'ai', 'confectionné', 'celle', 'de', '<EOS>']\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "-\n",
            "Input sentence: ['am', 'i', 'understood']\n",
            "Decoded sentence: ['suis', 'je', 'pas', '<EOS>']\n"
          ]
        }
      ],
      "source": [
        "################################################################################################\n",
        "\n",
        "\"\"\"\n",
        "You can now generate decoded sentences as such:\n",
        "\"\"\"\n",
        "\n",
        "def test():\n",
        "    for seq_index in range(2000, 2020):\n",
        "        # Take one sequence (part of the training set)\n",
        "        # for trying out decoding.\n",
        "        input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "        decoded_sentence = decode_sequence(input_seq)\n",
        "        print(\"-\")\n",
        "        print(\"Input sentence:\", input_texts[seq_index])\n",
        "        print(\"Decoded sentence:\", decoded_sentence)\n",
        "\n",
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4kHEJ3PFk2u",
        "outputId": "48d857fd-f8fb-4957-bd74-205804c31cbe"
      },
      "id": "F4kHEJ3PFk2u",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 8, 15910)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfa.seq2seq.BeamSearchDecoder(cell=decoder_lstm,beam_width=10,output_layer=decoder_dense)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "YnoPuB89Fni_",
        "outputId": "34fc4106-1fd0-441b-a3fb-a89405243fc2"
      },
      "id": "YnoPuB89Fni_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-fbd9bac07e5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeamSearchDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_lstm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeam_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_dense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/typeguard/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0mmemo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CallMemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_localns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mcheck_argument_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m         \u001b[0mcheck_return_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_addons/seq2seq/beam_search_decoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cell, beam_width, embedding_fn, output_layer, length_penalty_weight, coverage_penalty_weight, reorder_tensor_arrays, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m           \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         \"\"\"\n\u001b[0;32m--> 824\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    825\u001b[0m             \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m             \u001b[0mbeam_width\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/typeguard/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0mmemo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CallMemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_localns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mcheck_argument_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m         \u001b[0mcheck_return_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_addons/seq2seq/beam_search_decoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cell, beam_width, output_layer, length_penalty_weight, coverage_penalty_weight, reorder_tensor_arrays, output_all_scores, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m           \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \"\"\"\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mkeras_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_like_rnncell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cell\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/keras_utils.py\u001b[0m in \u001b[0;36massert_like_rnncell\u001b[0;34m(cell_name, cell)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconditions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0merror\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconditions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m    189\u001b[0m             \"The argument {!r} ({}) is not an RNNCell: {}.\".format(\n\u001b[1;32m    190\u001b[0m                 \u001b[0mcell_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: The argument 'cell' (<keras.layers.rnn.lstm.LSTM object at 0x7fce470a1d90>) is not an RNNCell: 'output_size' property is missing, 'state_size' property is missing."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def returntab(test_path,nMot,num_encoder_tokens,input_token_index,target_token_index):\n",
        "\n",
        "    batch_size = 64  # Batch size for training.\n",
        "    epochs = 2  # Number of epochs to train for.\n",
        "    latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "    num_samples = 10000  # Number of samples to train on.\n",
        "    # Path to the data txt file on disk.\n",
        "    data_path = \"fra.txt\"\n",
        "\n",
        "    \"\"\"\n",
        "    ## Prepare the data\n",
        "    \"\"\"\n",
        "\n",
        "    # Vectorize the data.\n",
        "    input_texts = []\n",
        "    target_texts = []\n",
        "    input_words = set()\n",
        "    target_words = set()\n",
        "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.read().split(\"\\n\")\n",
        "    for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "        input_text, target_text, _ = line.split(\"\\t\")\n",
        "        input_text =  [w for w in re.split(\"[^a-z0-9áàÀâÂçÇéÉèêÊëîïôÔœùûü']\", input_text.lower()) if w != '']\n",
        "        target_text = ['<BOS>'] + [w for w in re.split(\"[^a-z0-9áàÀâÂçÇéÉèêÊëîïôÔœùûü]\", target_text.lower()) if w != ''] + ['<EOS>']\n",
        "        input_texts.append(input_text)\n",
        "        target_texts.append(target_text)\n",
        "        for word in input_text:\n",
        "            if word not in input_words:\n",
        "                input_words.add(word)\n",
        "        for word in target_text:\n",
        "            if word not in target_words:\n",
        "                target_words.add(word)\n",
        "\n",
        "    max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "    max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_target_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "        for t, char in enumerate(input_text):\n",
        "            encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "        # encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "        for t, char in enumerate(target_text):\n",
        "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "            decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "            if t > 0:\n",
        "                # decoder_target_data will be ahead by one timestep\n",
        "                # and will not include the start character.\n",
        "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "\n",
        "    \"\"\"\n",
        "    ## Build the model\n",
        "    \"\"\"\n",
        "\n",
        "    # Define an input sequence and process it.\n",
        "    encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "    encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "    # We discard `encoder_outputs` and only keep the states.\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Set up the decoder, using `encoder_states` as initial state.\n",
        "    decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "    # We set up our decoder to return full output sequences,\n",
        "    # and to return internal states as well. We don't use the\n",
        "    # return states in the training model, but we will use them in inference.\n",
        "    decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Define the model that will turn\n",
        "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    \"\"\"\n",
        "    ## Train the model\n",
        "    \"\"\"\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    input_texts = []\n",
        "\n",
        "\n",
        "    with open(test_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.read().split(\"\\n\")\n",
        "    for line in lines:\n",
        "        input_text= line\n",
        "        input_text =  [w for w in re.split(\"[^a-z0-9áàÀâÂçÇéÉèêÊëîïôÔœùûü']\", input_text.lower()) if w != '']\n",
        "        input_texts.append(input_text)\n",
        "        for word in input_text:\n",
        "            if word not in input_words:\n",
        "                #input_words.add(word)\n",
        "                print(\"error :\" + word)\n",
        "\n",
        "\n",
        "\n",
        "    num_encoder_tokens = len(input_words)\n",
        "    max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "\n",
        "\n",
        "    input_token_index = dict([(char, i) for i, char in enumerate(input_words)])\n",
        "\n",
        "\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n",
        "\n",
        "\n",
        "    for i, input_text in enumerate(input_texts):\n",
        "        for t, char in enumerate(input_text):\n",
        "            encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    ## Run inference (sampling)\n",
        "\n",
        "    1. encode input and retrieve initial decoder state\n",
        "    2. run one step of decoder with this initial state\n",
        "    and a \"start of sequence\" token as target.\n",
        "    Output will be the next target token.\n",
        "    3. Repeat with the current target token and current states\n",
        "    \"\"\"\n",
        "\n",
        "    # Define sampling models\n",
        "    # Restore the model and construct the encoder and decoder.\n",
        "    model = keras.models.load_model(\"s2s\")\n",
        "\n",
        "    encoder_inputs = model.input[0]  # input_1\n",
        "    encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "    encoder_states = [state_h_enc, state_c_enc]\n",
        "    encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    decoder_inputs = model.input[1]  # input_2\n",
        "    decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "    decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_lstm = model.layers[3]\n",
        "    decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "        decoder_inputs, initial_state=decoder_states_inputs\n",
        "    )\n",
        "    decoder_states = [state_h_dec, state_c_dec]\n",
        "    decoder_dense = model.layers[4]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = keras.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        "    )\n",
        "\n",
        "    # Reverse-lookup token index to decode sequences back to\n",
        "    # something readable.\n",
        "    reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "    reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "    def decode_sequence(input_seq):\n",
        "        # Encode the input as state vectors.\n",
        "        states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "        # Generate empty target sequence of length 1.\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        # Populate the first character of target sequence with the start character.\n",
        "        target_seq[0, 0, target_token_index[\"<BOS>\"]] = 1.0\n",
        "\n",
        "        # Sampling loop for a batch of sequences\n",
        "        # (to simplify, here we assume a batch of size 1).\n",
        "        stop_condition = False\n",
        "        decoded_sentence = []\n",
        "        decoded_sentence_Bis = []\n",
        "        while not stop_condition:\n",
        "            output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        \n",
        "            # Sample a token\n",
        "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "            sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "\n",
        "            decoded_sentence_Bis.append(sampled_char)\n",
        "            #decoded_sentence.append(sampled_char)\n",
        "\n",
        "            # Exit condition: either hit max length\n",
        "            # or find stop character.\n",
        "            if sampled_char == \"<EOS>\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "                stop_condition = True\n",
        "\n",
        "            # Update the target sequence (of length 1).\n",
        "            target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "            target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "            # Update states\n",
        "            states_value = [h, c]\n",
        "\n",
        "            if sampled_char != \"<EOS>\":\n",
        "                l = list(np.argsort(output_tokens[0, -1, :]))\n",
        "                l.reverse()\n",
        "                l = l[:nMot]\n",
        "                lis = []\n",
        "                for i in l :\n",
        "                    lis.append(reverse_target_char_index[i])\n",
        "                decoded_sentence.append(lis)\n",
        "\n",
        "\n",
        "\n",
        "        return decoded_sentence,decoded_sentence_Bis\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    You can now generate decoded sentences as such:\n",
        "    \"\"\"\n",
        "    res = []\n",
        "\n",
        "    for i in range(0,len(encoder_input_data)):\n",
        "        # Take one sequence (part of the training set)\n",
        "        # for trying out decoding.\n",
        "        input_seq = encoder_input_data[i : i + 1]\n",
        "        decoded_sentence,decoded_sentence_Bis = decode_sequence(input_seq)\n",
        "        print(decoded_sentence_Bis)\n",
        "        res.append(decoded_sentence)\n",
        "    return res\n",
        "\n",
        "\n",
        "res = returntab(\"testTrad (1).txt\",10,num_encoder_tokens,input_token_index,target_token_index)\n",
        "print(res)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "Hi6rrppRT82e",
        "outputId": "fe29fc9c-8efa-47bd-a00a-2dd7cf54e384"
      },
      "id": "Hi6rrppRT82e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-055df26ce0d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturntab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"testTrad (1).txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_encoder_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_token_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_token_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-055df26ce0d4>\u001b[0m in \u001b[0;36mreturntab\u001b[0;34m(test_path, nMot, num_encoder_tokens, input_token_index, target_token_index)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mdecoder_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mdecoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     decoder_model = keras.Model(\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdecoder_states_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdecoder_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m                   for t in tf.nest.flatten(inputs)]):\n\u001b[1;32m    147\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_graph_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     nodes, nodes_by_depth, layers, _ = _map_graph_network(\n\u001b[0m\u001b[1;32m    233\u001b[0m         self.inputs, self.outputs)\n\u001b[1;32m    234\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m   1012\u001b[0m           \u001b[0;34mf'The name \"{name}\" is used {all_names.count(name)} '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m           'times in the model. All layer names should be unique.')\n",
            "\u001b[0;31mValueError\u001b[0m: The name \"input_4\" is used 2 times in the model. All layer names should be unique."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from itertools import combinations, product\n",
        "\n",
        "\"\"\"\n",
        "Title: Character-level recurrent sequence-to-sequence model\n",
        "Author: [fchollet](https://twitter.com/fchollet)\n",
        "Date created: 2017/09/29\n",
        "Last modified: 2020/04/26\n",
        "Description: Character-level recurrent sequence-to-sequence model.\n",
        "Accelerator: GPU\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "## Introduction\n",
        "\n",
        "This example demonstrates how to implement a basic character-level\n",
        "recurrent sequence-to-sequence model. We apply it to translating\n",
        "short English sentences into short French sentences,\n",
        "character-by-character. Note that it is fairly unusual to\n",
        "do character-level machine translation, as word-level\n",
        "models are more common in this domain.\n",
        "\n",
        "**Summary of the algorithm**\n",
        "\n",
        "- We start with input sequences from a domain (e.g. English sentences)\n",
        "    and corresponding target sequences from another domain\n",
        "    (e.g. French sentences).\n",
        "- An encoder LSTM turns input sequences to 2 state vectors\n",
        "    (we keep the last LSTM state and discard the outputs).\n",
        "- A decoder LSTM is trained to turn the target sequences into\n",
        "    the same sequence but offset by one timestep in the future,\n",
        "    a training process called \"teacher forcing\" in this context.\n",
        "    It uses as initial state the state vectors from the encoder.\n",
        "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
        "    given `targets[...t]`, conditioned on the input sequence.\n",
        "- In inference mode, when we want to decode unknown input sequences, we:\n",
        "    - Encode the input sequence into state vectors\n",
        "    - Start with a target sequence of size 1\n",
        "        (just the start-of-sequence character)\n",
        "    - Feed the state vectors and 1-char target sequence\n",
        "        to the decoder to produce predictions for the next character\n",
        "    - Sample the next character using these predictions\n",
        "        (we simply use argmax).\n",
        "    - Append the sampled character to the target sequence\n",
        "    - Repeat until we generate the end-of-sequence character or we\n",
        "        hit the character limit.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "## Setup\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## Download the data\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"shell\n",
        "!curl -O http://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip fra-eng.zip\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "## Configuration\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "def returntab(test_path, nMot):\n",
        "\n",
        "    batch_size = 64  # Batch size for training.\n",
        "    epochs = 2  # Number of epochs to train for.\n",
        "    latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "    num_samples = 10000  # Number of samples to train on.\n",
        "    # Path to the data txt file on disk.\n",
        "    data_path = \"fra.txt\"\n",
        "\n",
        "    \"\"\"\n",
        "    ## Prepare the data\n",
        "    \"\"\"\n",
        "\n",
        "    # Vectorize the data.\n",
        "    input_texts = []\n",
        "    target_texts = []\n",
        "    input_words = set()\n",
        "    target_words = set()\n",
        "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.read().split(\"\\n\")\n",
        "    for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "        input_text, target_text, _ = line.split(\"\\t\")\n",
        "        input_text = [w for w in re.split(\n",
        "            \"[^a-z0-9áàÀâÂçÇéÉèêÊëîïôÔœùûü']\", input_text.lower()) if w != '']\n",
        "        target_text = ['<BOS>'] + [w for w in re.split(\n",
        "            \"[^a-z0-9áàÀâÂçÇéÉèêÊëîïôÔœùûü]\", target_text.lower()) if w != ''] + ['<EOS>']\n",
        "        input_texts.append(input_text)\n",
        "        target_texts.append(target_text)\n",
        "        for word in input_text:\n",
        "            if word not in input_words:\n",
        "                input_words.add(word)\n",
        "        for word in target_text:\n",
        "            if word not in target_words:\n",
        "                target_words.add(word)\n",
        "\n",
        "    input_wordsSorted = sorted(list(input_words))\n",
        "    target_words = sorted(list(target_words))\n",
        "    num_encoder_tokens = len(input_wordsSorted)\n",
        "    num_decoder_tokens = len(target_words)\n",
        "    max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "    max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "    print(\"Number of samples:\", len(input_texts))\n",
        "    print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "    print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "    print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "    print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "    input_token_index = dict([(char, i)\n",
        "                             for i, char in enumerate(input_wordsSorted)])\n",
        "    target_token_index = dict([(char, i)\n",
        "                              for i, char in enumerate(target_words)])\n",
        "\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_target_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "        for t, char in enumerate(input_text):\n",
        "            encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "        # encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "        for t, char in enumerate(target_text):\n",
        "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "            decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "            if t > 0:\n",
        "                # decoder_target_data will be ahead by one timestep\n",
        "                # and will not include the start character.\n",
        "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "\n",
        "    \"\"\"\n",
        "    ## Build the model\n",
        "    \"\"\"\n",
        "\n",
        "    # Define an input sequence and process it.\n",
        "    encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "    encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "    # We discard `encoder_outputs` and only keep the states.\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Set up the decoder, using `encoder_states` as initial state.\n",
        "    decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "    # We set up our decoder to return full output sequences,\n",
        "    # and to return internal states as well. We don't use the\n",
        "    # return states in the training model, but we will use them in inference.\n",
        "    decoder_lstm = keras.layers.LSTM(\n",
        "        latent_dim, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(\n",
        "        decoder_inputs, initial_state=encoder_states)\n",
        "    decoder_dense = keras.layers.Dense(\n",
        "        num_decoder_tokens, activation=\"softmax\")\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Define the model that will turn\n",
        "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    \"\"\"\n",
        "    ## Train the model\n",
        "    \"\"\"\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    input_texts = []\n",
        "\n",
        "    with open(test_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.read().split(\"\\n\")\n",
        "    for line in lines:\n",
        "        input_text = line\n",
        "        input_text = [w for w in re.split(\n",
        "            \"[^a-z0-9áàÀâÂçÇéÉèêÊëîïôÔœùûü']\", input_text.lower()) if w != '']\n",
        "        input_texts.append(input_text)\n",
        "        for word in input_text:\n",
        "            if word not in input_words:\n",
        "                # input_words.add(word)\n",
        "                print(\"error :\" + word)\n",
        "\n",
        "    num_encoder_tokens = len(input_words)\n",
        "    max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "\n",
        "    input_token_index = dict([(char, i)\n",
        "                             for i, char in enumerate(input_wordsSorted)])\n",
        "\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n",
        "\n",
        "    for i, input_text in enumerate(input_texts):\n",
        "        for t, char in enumerate(input_text):\n",
        "            encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "\n",
        "    \"\"\"\n",
        "    ## Run inference (sampling)\n",
        "\n",
        "    1. encode input and retrieve initial decoder state\n",
        "    2. run one step of decoder with this initial state\n",
        "    and a \"start of sequence\" token as target.\n",
        "    Output will be the next target token.\n",
        "    3. Repeat with the current target token and current states\n",
        "    \"\"\"\n",
        "\n",
        "    # Define sampling models\n",
        "    # Restore the model and construct the encoder and decoder.\n",
        "    model = keras.models.load_model(\"s2s\")\n",
        "\n",
        "    encoder_inputs = model.input[0]  # input_1\n",
        "    # lstm_1\n",
        "    encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output\n",
        "    encoder_states = [state_h_enc, state_c_enc]\n",
        "    encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    decoder_inputs = model.input[1]  # input_2\n",
        "    decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "    decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_lstm = model.layers[3]\n",
        "    decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "        decoder_inputs, initial_state=decoder_states_inputs\n",
        "    )\n",
        "    decoder_states = [state_h_dec, state_c_dec]\n",
        "    decoder_dense = model.layers[4]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = keras.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        "    )\n",
        "\n",
        "    # Reverse-lookup token index to decode sequences back to\n",
        "    # something readable.\n",
        "    reverse_input_char_index = dict((i, char)\n",
        "                                    for char, i in input_token_index.items())\n",
        "    reverse_target_char_index = dict((i, char)\n",
        "                                     for char, i in target_token_index.items())\n",
        "\n",
        "    def decode_sequence(input_seq):\n",
        "        # Encode the input as state vectors.\n",
        "        states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "        # Generate empty target sequence of length 1.\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        # Populate the first character of target sequence with the start character.\n",
        "        target_seq[0, 0, target_token_index[\"<BOS>\"]] = 1.0\n",
        "\n",
        "        # Sampling loop for a batch of sequences\n",
        "        # (to simplify, here we assume a batch of size 1).\n",
        "        stop_condition = False\n",
        "        decoded_sentence = []\n",
        "        while not stop_condition:\n",
        "            output_tokens, h, c = decoder_model.predict(\n",
        "                [target_seq] + states_value)\n",
        "\n",
        "            # Sample a token\n",
        "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "            sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "            # decoded_sentence.append(sampled_char)\n",
        "\n",
        "            # Exit condition: either hit max length\n",
        "            # or find stop character.\n",
        "            if sampled_char == \"<EOS>\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "                stop_condition = True\n",
        "\n",
        "            # Update the target sequence (of length 1).\n",
        "            target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "            target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "            # Update states\n",
        "            states_value = [h, c]\n",
        "\n",
        "            if sampled_char != \"<EOS>\":\n",
        "                l = list(np.argsort(output_tokens[0, -1, :]))\n",
        "                l.reverse()\n",
        "                l = l[:nMot]\n",
        "                lis = []\n",
        "                for i in l:\n",
        "                    lis.append(reverse_target_char_index[i])\n",
        "                decoded_sentence.append(lis)\n",
        "\n",
        "        return decoded_sentence\n",
        "\n",
        "    \"\"\"\n",
        "    You can now generate decoded sentences as such:\n",
        "    \"\"\"\n",
        "    res = []\n",
        "\n",
        "    for i in range(0, len(encoder_input_data)):\n",
        "        # Take one sequence (part of the training set)\n",
        "        # for trying out decoding.\n",
        "        input_seq = encoder_input_data[i: i + 1]\n",
        "        decoded_sentence = decode_sequence(input_seq)\n",
        "        res.append(decoded_sentence)\n",
        "    return res\n",
        "\n",
        "\n",
        "res = returntab(\"testTrad.txt\", 5)\n",
        "\n",
        "print(res)\n",
        "\n",
        "\n",
        "\n",
        "def keep_first_strings(lists_of_strings):\n",
        "    first_strings = []\n",
        "    for sublist in lists_of_strings:\n",
        "        first_strings.append(sublist[0])\n",
        "    return first_strings\n",
        "\n",
        "def replace_last_strings(lists_of_strings, replacement_strings):\n",
        "    for i, sublist in enumerate(lists_of_strings):\n",
        "        sublist[-1] = replacement_strings[i]\n",
        "    return lists_of_strings\n",
        "\n",
        "\n",
        "\n",
        "def parse_file_to_array(file_name):\n",
        "    letters = []\n",
        "    with open(file_name, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            letters.extend(line.strip().split())\n",
        "    return letters\n",
        "\n",
        "letters = parse_file_to_array('./phonemes.txt')\n",
        "print(letters)\n",
        "\n",
        "\n",
        "def find_last_char_of_phonetic_of_line_ending_with_word(file_name, word, letters):\n",
        "    with open(file_name, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\"/\")\n",
        "            if parts[0].strip().endswith(word):\n",
        "              if parts[1][-2:] not in letters:\n",
        "                return parts[1][-1]\n",
        "              else:\n",
        "                return parts[1][-2:] #return the last character of the phonetic\n",
        "    return None\n",
        "\n",
        "def tableau_phonetic(arr):\n",
        "    i = 0\n",
        "    resultat = []\n",
        "    for sub_arr in arr:\n",
        "        resultat = resultat + [(sub_arr[-1])]\n",
        "    return resultat\n",
        "\n",
        "def replace_phonetic(lists):\n",
        "    return [[sublist,[find_last_char_of_phonetic_of_line_ending_with_word('./fr_FR.txt',x,letters) for x in sublist]] for sublist in lists]\n",
        "\n",
        "def check_rime_structure(arrays, type_rime):\n",
        "    array = arrays[1]\n",
        "    if type_rime == 'AAAA':\n",
        "        if array == [array[0]]*4:\n",
        "            return arrays[0]\n",
        "    elif type_rime == 'AABB':\n",
        "        if array[:2] == [array[0]]*2 and array[2:] == [array[2]]*2:\n",
        "            return arrays[0]\n",
        "    elif type_rime == 'ABAB':\n",
        "        if array[0] == array[2] and array[1] == array[3]:\n",
        "            return arrays[0]\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def find_phonetic_combinations(array, type_rime):\n",
        "    int_lists = tableau_phonetic(array)\n",
        "    combinations = list(product(*int_lists))\n",
        "    res = replace_phonetic(combinations)\n",
        "    resultat=[]\n",
        "    for i in res:\n",
        "      if check_rime_structure(i,type_rime):\n",
        "        resultat = resultat + i\n",
        "    temp = keep_first_strings(array)\n",
        "    print(len(resultat))\n",
        "    resultat = replace_last_strings(temp,resultat[0])\n",
        "    return resultat\n",
        "  \n",
        "\n",
        "find_last_char_of_phonetic_of_line_ending_with_word('./fr_FR.txt', 'prix', letters)"
      ],
      "metadata": {
        "id": "rFjfXHXPUkZj"
      },
      "id": "rFjfXHXPUkZj",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}